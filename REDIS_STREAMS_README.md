# Redis Streams Integration for Bank Statement Analyzer

This document describes the complete Redis Streams integration with AI categorization cache for the Bank Statement Analyzer application.

## ðŸš€ Overview

The Redis Streams integration provides:

- **Event-driven architecture** for statement processing
- **AI categorization cache** integration for optimal performance  
- **Scalable worker pools** for parallel processing
- **Real-time processing pipeline** from upload to final analysis
- **Comprehensive monitoring** and health checks
- **Production-ready deployment** with Docker and clustering

## ðŸ“‹ Architecture

### Streams and Workflows

```
STATEMENT_PROCESSING â†’ TRANSACTION_CATEGORIZATION â†’ RISK_ANALYSIS â†’ NOTIFICATIONS
                    â†“                             â†“                â†“
               AUDIT_LOG                    ALERTS           REPORTS
```

### Worker Types

1. **Statement Processing Workers** (2 instances)
   - Handle file upload processing
   - Parse statement files
   - Extract transactions
   - Data validation

2. **Transaction Categorization Workers** (3 instances)
   - AI-powered transaction categorization
   - Cache integration for performance
   - Batch processing capabilities
   - Real-time categorization

3. **Risk Analysis Workers** (2 instances)
   - Comprehensive risk assessment
   - Fraud pattern detection
   - Creditworthiness calculation
   - Cash flow analysis

## ðŸ›  Setup Instructions

### Prerequisites

- Node.js 18+
- Redis 7+
- MongoDB 7+
- Docker & Docker Compose (for containerized deployment)

### Local Development Setup

1. **Install dependencies:**
```bash
npm install
```

2. **Configure environment:**
```bash
cp .env.redis-streams .env
# Edit .env with your configuration
```

3. **Start Redis and MongoDB:**
```bash
# Using Docker
docker-compose -f docker-compose.redis-streams.yml up redis mongodb

# Or install locally
redis-server
mongod
```

4. **Start the application with Redis Streams:**
```bash
npm run redis-streams:dev
```

### Production Deployment

#### Option 1: Docker Compose (Recommended)

```bash
# Full stack with monitoring
docker-compose -f docker-compose.redis-streams.yml --profile monitoring up -d

# Core services only
docker-compose -f docker-compose.redis-streams.yml up -d

# With separate worker containers
docker-compose -f docker-compose.redis-streams.yml --profile workers-separate up -d
```

#### Option 2: Manual Deployment

```bash
# Production with clustering
npm run redis-streams:production

# Workers only (separate process)
npm run redis-streams:workers-only
```

## ðŸ”§ Configuration

### Environment Variables

```bash
# Core Configuration
NODE_ENV=production
PORT=3001
MONGODB_URI=mongodb://localhost:27017/bank-statement-analyzer
REDIS_URL=redis://localhost:6379

# Worker Configuration
ENABLE_WORKERS=true
ENABLE_CLUSTERING=false
MAX_WORKERS=4
STATEMENT_WORKERS=2
CATEGORIZATION_WORKERS=3
RISK_WORKERS=2

# AI Cache Configuration
AI_CACHE_ENABLED=true
AI_CACHE_TTL=86400
AI_CACHE_MAX_SIZE=10000
```

### Worker Scaling

Workers can be scaled based on load:

```bash
# Scale categorization workers for high load
CATEGORIZATION_WORKERS=5 npm run redis-streams

# Enable clustering for multi-core systems
ENABLE_CLUSTERING=true npm run redis-streams:cluster
```

## ðŸ“Š Monitoring & Health Checks

### Health Check Endpoint

```bash
curl http://localhost:3001/health
```

Response:
```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:00:00.000Z",
  "services": {
    "mongodb": "connected",
    "redis": "connected",
    "workers": "running"
  },
  "workers": {
    "statement-0": {
      "type": "statementProcessing",
      "status": "running",
      "processedCount": 150,
      "errorCount": 2
    }
  }
}
```

### Metrics Endpoint

```bash
curl http://localhost:3001/metrics
```

### Redis Monitoring

Access Redis Commander at `http://localhost:8081` (when using monitoring profile)

### MongoDB Monitoring

Access Mongo Express at `http://localhost:8082` (when using monitoring profile)

## ðŸ”„ Processing Flow

### 1. Statement Upload

```javascript
POST /api/statements/upload
```

**Flow:**
1. File uploaded via multer
2. Statement saved to MongoDB
3. Queued to `STATEMENT_PROCESSING` stream
4. Queued to `TRANSACTION_CATEGORIZATION` stream
5. Returns immediately with statement ID

### 2. Background Processing

**Statement Processing Worker:**
1. Parses uploaded file
2. Extracts transactions
3. Validates data
4. Triggers categorization

**Categorization Worker:**
1. Processes transactions with AI cache
2. Batch categorization for efficiency
3. Updates transaction categories
4. Triggers risk analysis

**Risk Analysis Worker:**
1. Comprehensive risk assessment
2. Fraud pattern detection
3. Generates risk reports
4. Sends alerts if needed

### 3. Real-time Status

```javascript
GET /api/statements/:id/status
```

Returns processing status and real-time updates.

## ðŸŽ¯ AI Categorization Cache Integration

### Cache Strategy

- **Cache Key:** Hash of transaction description + amount + merchant
- **Cache TTL:** 24 hours (configurable)
- **Cache Miss:** Falls back to AI categorization
- **Batch Processing:** Optimized for high-throughput scenarios

### Performance Benefits

- **90%+ cache hit rate** for common transactions
- **10x faster** categorization for cached transactions
- **Reduced AI API costs** through intelligent caching
- **Improved response times** for end users

### Cache Management

```javascript
// Manual cache operations
GET /api/cache/stats
DELETE /api/cache/clear
POST /api/cache/preload
```

## ðŸš¨ Error Handling & Recovery

### Retry Logic

- **Exponential backoff** for failed jobs
- **Dead letter queues** for persistent failures
- **Manual retry** capabilities for administrators

### Monitoring Alerts

- Stream length monitoring
- Worker health checks
- Error rate thresholds
- Performance degradation alerts

## ðŸ”’ Security Considerations

### Stream Security

- Consumer group authentication
- Stream access controls
- Message encryption (optional)
- Audit logging for all operations

### Worker Security

- Isolated worker processes
- Limited database permissions
- Secure inter-service communication
- Environment variable protection

## ðŸ“ˆ Performance Tuning

### Worker Optimization

```bash
# High-throughput configuration
CATEGORIZATION_WORKERS=5
RISK_WORKERS=3
ENABLE_CLUSTERING=true
MAX_WORKERS=8
```

### Redis Optimization

```bash
# Redis configuration for high load
maxmemory 2gb
maxmemory-policy allkeys-lru
save 900 1
save 300 10
save 60 10000
```

### MongoDB Optimization

```javascript
// Recommended indexes
db.statements.createIndex({ userId: 1, createdAt: -1 })
db.transactions.createIndex({ statementId: 1, category: 1 })
db.transactions.createIndex({ userId: 1, date: -1 })
```

## ðŸ§ª Testing

### Unit Tests

```bash
npm run test:unit
```

### Integration Tests

```bash
npm run test:integration
```

### Load Testing

```bash
# Test worker performance
npm run test:load:workers

# Test stream throughput
npm run test:load:streams
```

## ðŸ“š API Reference

### Stream Events

#### Statement Processing Stream

```javascript
{
  type: 'PROCESS_UPLOADED_STATEMENT',
  payload: {
    statementId: 'string',
    filePath: 'string',
    userId: 'string',
    uploadMetadata: object
  }
}
```

#### Categorization Stream

```javascript
{
  type: 'CATEGORIZE_STATEMENT_TRANSACTIONS',
  payload: {
    statementId: 'string',
    userId: 'string',
    nextStage: 'ANALYSIS'
  }
}
```

#### Risk Analysis Stream

```javascript
{
  type: 'ANALYZE_STATEMENT_RISK',
  payload: {
    statementId: 'string',
    userId: 'string',
    transactionCount: number
  }
}
```

## ðŸ”§ Troubleshooting

### Common Issues

1. **Redis Connection Failed**
   ```bash
   # Check Redis status
   redis-cli ping
   
   # Restart Redis
   docker-compose restart redis
   ```

2. **Workers Not Processing**
   ```bash
   # Check worker status
   curl http://localhost:3001/health
   
   # Restart workers
   docker-compose restart api
   ```

3. **High Memory Usage**
   ```bash
   # Check stream lengths
   curl http://localhost:3001/metrics
   
   # Clear processed messages
   redis-cli XTRIM statement:processing MAXLEN 1000
   ```

### Debug Mode

```bash
# Enable debug logging
LOG_LEVEL=debug npm run redis-streams:dev

# Worker-specific debugging
DEBUG=worker:* npm run workers:categorization
```

## ðŸš€ Future Enhancements

- **Auto-scaling** based on queue length
- **Multi-region deployment** with stream replication
- **Real-time dashboards** for monitoring
- **Advanced AI models** for categorization
- **Machine learning** pipeline integration

## ðŸ“ž Support

For issues and questions:

1. Check the health endpoint: `/health`
2. Review application logs: `./logs/`
3. Monitor Redis streams: Redis Commander
4. Check worker status: `/metrics`

## ðŸ“„ License

This Redis Streams integration is part of the Bank Statement Analyzer project.
